# #cython: language_level=3
from numba import njit
from numba.experimental import jitclass

import numpy as np

import warnings

# from scipy.optimize.minpack2 import dcsrch

# from scipy.optimize.minpack2 import dcsrch


_status_message = {'success': 'Optimization terminated successfully.',
                   'maxiter': 'Maximum number of iterations has been '
                              'exceeded.',
                   'pr_loss': 'Desired error not necessarily achieved due '
                              'to precision loss.',
                   'nan': 'NaN result encountered.'}


def _minimize_bfgs(fun, x0, maxiter=None, retall=False, gtol=1e-5, disp=True, full_output=True):

    if maxiter is None:
        maxiter = 200 * len(x0)

    sf = ScalarFunction(fun, x0)

    f = sf.fun
    myfprime = sf.grad

    old_fval = f(x0)
    gfk = myfprime(x0)

    if not np.isscalar(old_fval):
        try:
            old_fval = old_fval.item()
        except (ValueError, AttributeError) as e:
            raise ValueError("The user-provided "
                             "objective function must "
                             "return a scalar value.") from e

    k = 0
    N = len(x0)
    I = np.eye(N, dtype=int)
    Hk = I

    # Sets the initial step guess to dx ~ 1
    old_old_fval = old_fval + np.linalg.norm(gfk) / 2

    xk = x0
    warnflag = 0
    gnorm = np.amax(np.abs(gfk))
    while (gnorm > gtol) and (k < maxiter):
        pk = -np.dot(Hk, gfk)
        try:
            alpha_k, fc, gc, old_fval, old_old_fval, gfkp1 = \
                     _line_search_wolfe12(f, myfprime, xk, pk, gfk,
                                          old_fval, old_old_fval, amin=1e-100, amax=1e100)
        except _LineSearchError:
            # Line search failed to find a better solution.
            warnflag = 2
            break

        xkp1 = xk + alpha_k * pk
        sk = xkp1 - xk
        xk = xkp1
        if gfkp1 is None:
            gfkp1 = myfprime(xkp1)

        yk = gfkp1 - gfk
        gfk = gfkp1
        
        k += 1
        gnorm = np.amax(np.abs(gfk))
        if (gnorm <= gtol):
            break

        if not np.isfinite(old_fval):
            # We correctly found +-Inf as optimal value, or something went
            # wrong.
            warnflag = 2
            break

        rhok_inv = np.dot(yk, sk)
        # this was handled in numeric, let it remaines for more safety
        if rhok_inv == 0.:
            rhok = 1000.0
            if disp:
                print("Divide-by-zero encountered: rhok assumed large")
        else:
            rhok = 1. / rhok_inv

        A1 = I - sk[:, np.newaxis] * yk[np.newaxis, :] * rhok
        A2 = I - yk[:, np.newaxis] * sk[np.newaxis, :] * rhok
        Hk = np.dot(A1, np.dot(Hk, A2)) + (rhok * sk[:, np.newaxis] *
                                                 sk[np.newaxis, :])

    fval = old_fval

    if warnflag == 2:
        msg = _status_message['pr_loss']
    elif k >= maxiter:
        warnflag = 1
        msg = _status_message['maxiter']
    elif np.isnan(gnorm) or np.isnan(fval) or np.isnan(xk).any():
        warnflag = 3
        msg = _status_message['nan']
    else:
        msg = _status_message['success']

    if full_output:
        return xk, fval
    return xk

# @jitclass
class ScalarFunction(object):

    def __init__(self, fun, x0):
        self.x = x0
        self.n = self.x.size
        self.nfev = 0
        self.ngev = 0
        self.f_updated = False
        self.g_updated = False

        # Function evaluation
        def fun_wrapped(x):
            self.nfev += 1
            return fun(x)

        def update_fun():
            self.f = fun_wrapped(self.x)

        self._update_fun_impl = update_fun
        self._update_fun()

        def update_grad():
            self._update_fun()
            self.ngev += 1
            self.g = approx_derivative(fun_wrapped, self.x, self.f)

        self._update_grad_impl = update_grad
        self._update_grad()

        def update_x(x):
            self.x = np.atleast_1d(x).astype(float)
            self.f_updated = False
            self.g_updated = False
            
        self._update_x_impl = update_x

    def _update_fun(self):
        if not self.f_updated:
            self._update_fun_impl()
            self.f_updated = True

    def _update_grad(self):
        if not self.g_updated:
            self._update_grad_impl()
            self.g_updated = True

    def fun(self, x):
        if not np.array_equal(x, self.x):
            self._update_x_impl(x)
        self._update_fun()
        return self.f

    def grad(self, x):
        if not np.array_equal(x, self.x):
            self._update_x_impl(x)
        self._update_grad()
        return self.g

    def fun_and_grad(self, x):
        if not np.array_equal(x, self.x):
            self._update_x_impl(x)
        self._update_fun()
        self._update_grad()
        return self.f, self.g

@njit
def _dense_difference(fun, x0, f0, h, use_one_sided):
    m = f0.size
    n = x0.size
    J_transposed = np.empty((n, m))
    h_vecs = np.diag(h)

    for i in range(h.size):
        if use_one_sided[i]:
            x1 = x0 + h_vecs[i]
            x2 = x0 + 2 * h_vecs[i]
            dx = x2[i] - x0[i]
            f1 = fun(x1)
            f2 = fun(x2)
            df = -3.0 * f0 + 4 * f1 - f2
        else:
            x1 = x0 - h_vecs[i]
            x2 = x0 + h_vecs[i]
            dx = x2[i] - x1[i]
            f1 = fun(x1)
            f2 = fun(x2)
            df = f2 - f1

        J_transposed[i] = df / dx

    if m == 1:
        J_transposed = np.ravel(J_transposed)

    return J_transposed.T


@njit
def approx_derivative(fun, x0, f0, bounds=(-np.inf, np.inf)):

    lb = np.resize(bounds[0], x0.shape)
    ub = np.resize(bounds[1], x0.shape)

    # f0 = fun(x0)
    f0 = np.atleast_1d(f0)

    # by default we use rel_step
    EPS = np.finfo(np.float64).eps**(1/3)
    sign_x0 = (x0 >= 0).astype(float) * 2 - 1
    h = EPS * sign_x0 * np.maximum(1.0, np.abs(x0))
    h = np.abs(h)
    use_one_sided = np.zeros_like(h, dtype=bool)
    
    return _dense_difference(fun, x0, f0, h, use_one_sided)


class _LineSearchError(RuntimeError):
    pass


class LineSearchWarning(RuntimeWarning):
    pass
    
# from edpyt.minpack2 import dcsrch
from edpyt.dcsrch_wrap import dcsrch

# @njit
def scalar_search_wolfe1(phi, derphi, phi0, old_phi0, derphi0,
                         c1=1e-4, c2=0.9,
                         amax=50, amin=1e-8, xtol=1e-14):

    if old_phi0 is not None and derphi0 != 0.:
        alpha1 = min(1.0, 1.01*2*(phi0 - old_phi0)/derphi0)
        if alpha1 < 0:
            alpha1 = 1.0
    else:
        alpha1 = 1.0

    phi1 = phi0
    derphi1 = derphi0
    isave = np.zeros((2,), np.int32)
    dsave = np.zeros((13,), np.float64)
    # task = b'START'
    task = np.array([ord(i) for i in 'START                                                       '], np.int8)
    FG = np.array([ord(i) for i in 'FG'], np.int8)
    ERROR = np.array([ord(i) for i in 'ERROR'], np.int8)
    WARN = np.array([ord(i) for i in 'WARN'], np.int8)

    maxiter = 100
    # stp = alpha1; f = phi1; g = derphi1
    for i in range(maxiter):
        stp, phi1, derphi1, task = dcsrch(alpha1, phi1, derphi1,
                                          c1, c2, xtol, task,
                                          amin, amax, isave, dsave)
        # dcsrch(alpha1, phi1, derphi1,
        #        c1, c2, xtol, task,
        #        amin, amax, isave, dsave)
        # if task[:2] == b'FG':
        if np.allclose(FG,task[:2]):
            alpha1 = stp
            phi1 = phi(stp)
            derphi1 = derphi(stp)
        else:
            break
    else:
        # maxiter reached, the line search did not converge
        stp = None

    # if task[:5] == b'ERROR' or task[:4] == b'WARN':
    if np.allclose(task[:5],ERROR) or np.allclose(task[:4],WARN):
        stp = None  # failed

    return stp, phi1, phi0

# @njit
def line_search_wolfe1(f, fprime, xk, pk, gfk, old_fval, old_old_fval, 
                       c1=1e-4, c2=0.9, 
                       amax=50, amin=1e-8, xtol=1e-14):

    gval = [gfk]
    gc = [0]
    fc = [0]

    # @njit
    def phi(s):
        fc[0] += 1
        return f(xk + s*pk)

    # @njit
    def derphi(s):
        gval[0] = fprime(xk + s*pk)
        gc[0] += 1
        return np.dot(gval[0], pk)

    derphi0 = np.dot(gfk, pk)

    stp, fval, old_fval = scalar_search_wolfe1(
            phi, derphi, old_fval, old_old_fval, derphi0,
            c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)

    return stp, fc[0], gc[0], fval, old_fval, gval[0]

# @njit
def scalar_search_wolfe2(phi, derphi, phi0, old_phi0, derphi0,
                         c1=1e-4, c2=0.9, amax=None, maxiter=10):

    alpha0 = 0
    if old_phi0 is not None and derphi0 != 0:
        alpha1 = min(1.0, 1.01*2*(phi0 - old_phi0)/derphi0)
    else:
        alpha1 = 1.0

    if alpha1 < 0:
        alpha1 = 1.0

    if amax is not None:
        alpha1 = min(alpha1, amax)

    phi_a1 = phi(alpha1)
    #derphi_a1 = derphi(alpha1) evaluated below

    phi_a0 = phi0
    derphi_a0 = derphi0

    for i in range(maxiter):
        if alpha1 == 0 or (amax is not None and alpha0 == amax):
            # alpha1 == 0: This shouldn't happen. Perhaps the increment has
            # slipped below machine precision?
            alpha_star = None
            phi_star = phi0
            phi0 = old_phi0
            derphi_star = None

            if alpha1 == 0:
                msg = 'Rounding errors prevent the line search from converging'
            else:
                msg = "The line search algorithm could not find a solution " + \
                      "less than or equal to amax: %s" % amax

            warnings.warn(msg, LineSearchWarning)
            break

        not_first_iteration = i > 0
        if (phi_a1 > phi0 + c1 * alpha1 * derphi0) or \
           ((phi_a1 >= phi_a0) and not_first_iteration):
            alpha_star, phi_star, derphi_star = \
                        _zoom(alpha0, alpha1, phi_a0,
                              phi_a1, derphi_a0, phi, derphi,
                              phi0, derphi0, c1, c2)
            break

        derphi_a1 = derphi(alpha1)
        if (abs(derphi_a1) <= -c2*derphi0):
            alpha_star = alpha1
            phi_star = phi_a1
            derphi_star = derphi_a1
            break

        if (derphi_a1 >= 0):
            alpha_star, phi_star, derphi_star = \
                        _zoom(alpha1, alpha0, phi_a1,
                              phi_a0, derphi_a1, phi, derphi,
                              phi0, derphi0, c1, c2)
            break

        alpha2 = 2 * alpha1  # increase by factor of two on each iteration
        if amax is not None:
            alpha2 = min(alpha2, amax)
        alpha0 = alpha1
        alpha1 = alpha2
        phi_a0 = phi_a1
        phi_a1 = phi(alpha1)
        derphi_a0 = derphi_a1

    else:
        # stopping test maxiter reached
        alpha_star = alpha1
        phi_star = phi_a1
        derphi_star = None
        warnings.warn('The line search algorithm did not converge', LineSearchWarning)

    return alpha_star, phi_star, phi0, derphi_star

# @njit
def _cubicmin(a, fa, fpa, b, fb, c, fc):
    """
    Finds the minimizer for a cubic polynomial that goes through the
    points (a,fa), (b,fb), and (c,fc) with derivative at a of fpa.
    If no minimizer can be found, return None.
    """
    # f(x) = A *(x-a)^3 + B*(x-a)^2 + C*(x-a) + D

    with np.errstate(divide='raise', over='raise', invalid='raise'):
        try:
            C = fpa
            db = b - a
            dc = c - a
            denom = (db * dc) ** 2 * (db - dc)
            d1 = np.empty((2, 2))
            d1[0, 0] = dc ** 2
            d1[0, 1] = -db ** 2
            d1[1, 0] = -dc ** 3
            d1[1, 1] = db ** 3
            [A, B] = np.dot(d1, np.asarray([fb - fa - C * db,
                                            fc - fa - C * dc]).flatten())
            A /= denom
            B /= denom
            radical = B * B - 3 * A * C
            xmin = a + (-B + np.sqrt(radical)) / (3 * A)
        except ArithmeticError:
            return None
    if not np.isfinite(xmin):
        return None
    return xmin

# @njit
def _quadmin(a, fa, fpa, b, fb):
    """
    Finds the minimizer for a quadratic polynomial that goes through
    the points (a,fa), (b,fb) with derivative at a of fpa.
    """
    # f(x) = B*(x-a)^2 + C*(x-a) + D
    with np.errstate(divide='raise', over='raise', invalid='raise'):
        try:
            D = fa
            C = fpa
            db = b - a * 1.0
            B = (fb - D - C * db) / (db * db)
            xmin = a - C / (2.0 * B)
        except ArithmeticError:
            return None
    if not np.isfinite(xmin):
        return None
    return xmin

# @njit
def _zoom(a_lo, a_hi, phi_lo, phi_hi, derphi_lo,
          phi, derphi, phi0, derphi0, c1, c2):
    """Zoom stage of approximate linesearch satisfying strong Wolfe conditions.
    
    Part of the optimization algorithm in `scalar_search_wolfe2`.
    
    Notes
    -----
    Implements Algorithm 3.6 (zoom) in Wright and Nocedal,
    'Numerical Optimization', 1999, pp. 61.
    """

    maxiter = 10
    i = 0
    delta1 = 0.2  # cubic interpolant check
    delta2 = 0.1  # quadratic interpolant check
    phi_rec = phi0
    a_rec = 0
    while True:
        # interpolate to find a trial step length between a_lo and
        # a_hi Need to choose interpolation here. Use cubic
        # interpolation and then if the result is within delta *
        # dalpha or outside of the interval bounded by a_lo or a_hi
        # then use quadratic interpolation, if the result is still too
        # close, then use bisection

        dalpha = a_hi - a_lo
        if dalpha < 0:
            a, b = a_hi, a_lo
        else:
            a, b = a_lo, a_hi

        # minimizer of cubic interpolant
        # (uses phi_lo, derphi_lo, phi_hi, and the most recent value of phi)
        #
        # if the result is too close to the end points (or out of the
        # interval), then use quadratic interpolation with phi_lo,
        # derphi_lo and phi_hi if the result is still too close to the
        # end points (or out of the interval) then use bisection

        if (i > 0):
            cchk = delta1 * dalpha
            a_j = _cubicmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi,
                            a_rec, phi_rec)
        if (i == 0) or (a_j is None) or (a_j > b - cchk) or (a_j < a + cchk):
            qchk = delta2 * dalpha
            a_j = _quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)
            if (a_j is None) or (a_j > b-qchk) or (a_j < a+qchk):
                a_j = a_lo + 0.5*dalpha

        # Check new value of a_j

        phi_aj = phi(a_j)
        if (phi_aj > phi0 + c1*a_j*derphi0) or (phi_aj >= phi_lo):
            phi_rec = phi_hi
            a_rec = a_hi
            a_hi = a_j
            phi_hi = phi_aj
        else:
            derphi_aj = derphi(a_j)
            if abs(derphi_aj) <= -c2*derphi0:
                a_star = a_j
                val_star = phi_aj
                valprime_star = derphi_aj
                break
            if derphi_aj*(a_hi - a_lo) >= 0:
                phi_rec = phi_hi
                a_rec = a_hi
                a_hi = a_lo
                phi_hi = phi_lo
            else:
                phi_rec = phi_lo
                a_rec = a_lo
            a_lo = a_j
            phi_lo = phi_aj
            derphi_lo = derphi_aj
        i += 1
        if (i > maxiter):
            # Failed to find a conforming step size
            a_star = None
            val_star = None
            valprime_star = None
            break
    return a_star, val_star, valprime_star

# @njit
def line_search_wolfe2(f, myfprime, xk, pk, gfk, old_fval, old_old_fval, 
                       c1=1e-4, c2=0.9,
                       amax=None, maxiter=10):
    fc = [0]
    gc = [0]
    gval = [None]
    gval_alpha = [None]

    # @njit
    def phi(alpha):
        fc[0] += 1
        return f(xk + alpha * pk)

    fprime = myfprime

    # @njit
    def derphi(alpha):
        gc[0] += 1
        gval[0] = fprime(xk + alpha * pk)  # store for later use
        gval_alpha[0] = alpha
        return np.dot(gval[0], pk)

    derphi0 = np.dot(gfk, pk)

    alpha_star, phi_star, old_fval, derphi_star = scalar_search_wolfe2(
            phi, derphi, old_fval, old_old_fval, derphi0, c1, c2, amax,
            maxiter=maxiter)

    if derphi_star is None:
        warnings.warn('The line search algorithm did not converge', LineSearchWarning)
    else:
        # derphi_star is a number (derphi) -- so use the most recently
        # calculated gradient used in computing it derphi = gfk*pk
        # this is the gradient at the next step no need to compute it
        # again in the outer loop.
        derphi_star = gval[0]

    return alpha_star, fc[0], gc[0], phi_star, old_fval, derphi_star


def _line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval, amin=1e-100, amax=1e100):

    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,
                             old_fval, old_old_fval, amin=amin, amax=amax)

    if ret[0] is None:
        # line search failed: try different one.
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', LineSearchWarning)
            ret = line_search_wolfe2(f, fprime, xk, pk, gfk,
                                     old_fval, old_old_fval, amax=amax)

    if ret[0] is None:
        raise _LineSearchError()

    return ret


if __name__ == '__main__':

    from scipy.optimize import fmin_bfgs

    @njit
    def f(x):
        return (x[0] - 1)**2 + (x[1] - 2.5)**2
    
    xopt, eps = _minimize_bfgs(f, np.random.random(2), full_output=True)
    expected, eps = fmin_bfgs(f, np.random.random(2), full_output=True)[:2]
    np.testing.assert_allclose(xopt, expected)